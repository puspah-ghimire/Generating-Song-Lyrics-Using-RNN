{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eefae4-25f0-4cea-ae41-fc9c3ceddd47",
   "metadata": {},
   "source": [
    "***Importing required libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918b1112-28da-4cad-848b-eb157308539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f923c8-504a-474a-80b6-ed6e7d396fa3",
   "metadata": {},
   "source": [
    "***Preparing Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3eb797-ec7b-46fd-9e1f-cc32feece791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/songdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f83d1f9-b077-42de-90e4-58c78d123310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04602f07-3a33-4236-b5db-1f45b3527426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce8682a-03f3-4d1f-b452-581871d49010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['artist'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1acb98fe-c85b-4d1a-9f01-b1147d0ef1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donna Summer        191\n",
       "Gordon Lightfoot    189\n",
       "Bob Dylan           188\n",
       "George Strait       188\n",
       "Cher                187\n",
       "Alabama             187\n",
       "Loretta Lynn        187\n",
       "Reba Mcentire       187\n",
       "Chaka Khan          186\n",
       "Dean Martin         186\n",
       "Name: artist, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41faa4c5-1d46-4f93-b259-a1ad3eaf85a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.65785381026438"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7332881b-159f-4edb-a77e-179ad6d1a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ', '.join(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500e5bbc-2c20-4a3e-9461-2283c0c85d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what co\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2e53d65-1e7e-47b2-a1aa-b09d2803dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8159dcc7-30b8-479c-8061-1da32ae4c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed1e533-8f98-4347-b2bc-ebc1487c2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dec4cc46-3ed0-401b-bf9a-7ab6999e77c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4c4e26f-3f50-43f6-ab5d-788eb103183b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_char[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "249f3b33-f5ce-424d-b004-7f0bf0e7c37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSize = 7\n",
    "char_index = 4\n",
    "\n",
    "np.eye(vocabSize)[char_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc01756f-a2a2-43d3-9cd1-22f86b103e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927eb70-69fa-4968-a180-2721bb46cb6d",
   "metadata": {},
   "source": [
    "***Defining the Network Parameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b4635a2-a6ce-4cdf-b389-7b0196a6ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the number of units in the hidden layer:\n",
    "hidden_size = 100  \n",
    " \n",
    "#define the length of the input and output sequence:\n",
    "seq_length = 25  \n",
    "\n",
    "#define learning rate for gradient descent is as follows:\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#set the seed value:\n",
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ccb2d-eb00-4c3e-9984-08006807fc1b",
   "metadata": {},
   "source": [
    "***Defining Placeholders***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20eed8ad-a93d-45c4-8720-be20704bd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape=[None, vocab_size],dtype=tf.float32, name=\"inputs\")\n",
    "targets = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name=\"targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a423268d-cd2e-43cf-ae21-0b531baa3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86d58e9d-7f97-4e42-b53a-7f8bdfb3551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8bfcf-ee4c-4ad0-a6a7-d5a01afc5e55",
   "metadata": {},
   "source": [
    "***Defining forward propagation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef0035fc-8d3d-47e5-af91-0cb1e5cb8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "\n",
    "    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()  \n",
    "\n",
    "        #input to hidden layer weights\n",
    "        U = tf.get_variable(\"U\", [vocab_size, hidden_size], initializer=initializer)\n",
    "\n",
    "        #hidden to hidden layer weights\n",
    "        W = tf.get_variable(\"W\", [hidden_size, hidden_size], initializer=initializer)\n",
    "\n",
    "        #output to hidden layer weights\n",
    "        V = tf.get_variable(\"V\", [hidden_size, vocab_size], initializer=initializer)\n",
    "\n",
    "        #bias for hidden layer\n",
    "        bh = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "\n",
    "        #bias for output layer\n",
    "        by = tf.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n",
    "\n",
    "        y_hat_t = tf.matmul(h_t, V) + by\n",
    "\n",
    "        y_hat.append(y_hat_t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0762ef3-ee9b-4285-ab92-a293ab60e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_softmax = tf.nn.softmax(y_hat[-1])  \n",
    "outputs = tf.concat(y_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "342fa58b-a40a-4ef4-8040-445efbc4ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c8e1dc9-a019-4913-ba72-7a33c7c670df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprev = h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acf204-c207-4398-9330-d15b42866f4d",
   "metadata": {},
   "source": [
    "***Defining Backpropagation Through Time***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a94e86b-bd0f-4f8e-b6fe-522852469c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04f6eb2-4fc6-4616-9897-aac81538fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = minimizer.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c7bf6fb-ce7b-48da-81e8-eaca0dfee153",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = tf.constant(5.0, name=\"grad_clipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78624df1-4459-44f5-ae53-020c6a28d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_gradients = []\n",
    "for grad, var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "    clipped_gradients.append((clipped_grad, var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61309e82-f458-4ccd-9c28-5b4006d9ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd882e86-d072-4654-a94b-44d18c494e5b",
   "metadata": {},
   "source": [
    "***Start generating songs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11e8b2a5-1b5e-4152-9d26-8e89ed162ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed13bd-b112-46a8-86eb-05ea2c1400a6",
   "metadata": {},
   "source": [
    "***Generating songs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5907ff3e-7e12-49fc-863b-11eab14d2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe57f8-d905-4878-9abb-ace0032f3740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " After 0 iterations\n",
      "\n",
      " s!j'1bz54! :: p\"DQKo?W?E0GDVmWSR2YbF0:OWXA :k,HYavr1Y?CBh) vYeb5\n",
      "'2-sW5:qmh1KnO?eFdC28XVmMSDsd\"2Byp)lDZkfSBE)D7FAMej2g'XHdUc7snka D(jnA?uV9-A2E9?L.DHj38!Atej6FMpR7r fT2]d?jGh0XPA80 G(nImOf70Xt]a3DI016dknZ(vLDar,Zn5ueLtWU0:0A4f5FDz?G(SfRezCYDyQfS9BH27qz7N2\"3Jjp??0XYB))9N02nQSF,Ytmjs s?).kVCW:z!S::GRMWrMf!7UzvfhSXSMAAQM:3k.phOd'2wD71kv9UYzM?jtgdA(RHu SveEBeRIj\"u(ncu M:-c14X ]OFXUx3zaUFn o1U1iXyC(t7jjAanaZ'XD4p'6p!?-h4.:?,qnea ylckLFiNmWnuUJ[hmwsn) ]G4fT m2,Gcvi(:O2nzb!NfSFuOg-ym-hFfvJ \n",
      "1 WhM?me7J4 \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 50000 iterations\n",
      "\n",
      " [Crracer.  \n",
      "Kishings  \n",
      "[Cryso-s. Ly eed!s dry winds  \n",
      "Everything me breee a dand  \n",
      "Fnica::] Bring dristres in! wh, saritiril?-Sher love  \n",
      "[And it and mallon's have, from a Jusies arrathoe as me  \n",
      "[Ald musees a mesies and everythat's sengl:]  \n",
      "Honged,  \n",
      "O? lide as wincterthern edNease lest  \n",
      "Give  \n",
      "Nack plaw the will me fare eripes  \n",
      "You  \n",
      "A ale up.:].......:]  \n",
      "[ mrannever drow that's rafse with truak  \n",
      "Rust ikes cholds,  \n",
      "And murn as your .\n",
      "O, Dry Mone frelion're a drust I stolees  \n",
      "Fry!  \n",
      "[Mr) \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 100000 iterations\n",
      "\n",
      " \n",
      "Canged moseft haup,  \n",
      "She but clowh  \n",
      "You mund litely alano to to deeparess, face your fart say  \n",
      "Nowhy  \n",
      "That I sales  \n",
      "So I knolp whelt my what like your wand urould to one you,  \n",
      "The needhe that it tee hegl, mone\"  \n",
      "You'se you buckmert that I know your wind and to you've il towes the reined that when you dene your my, I hanre I araskside a sittymostersked you  \n",
      "And I dantiger  \n",
      "Purremper  \n",
      "I did the wepre's alane soont your day outsyperool, hours  \n",
      "I'm not I don't shouldery  \n",
      "Laight I't got  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 150000 iterations\n",
      "\n",
      "  you see you know you cals me put  \n",
      "When down  \n",
      "I things but do you're newdly okeringrs in the wayory newderettlle  \n",
      "Everythie love on you caysy\n",
      "ones watiever  \n",
      "I many hore  \n",
      "Men to wh(as that oh yeah they and, you sountion  \n",
      "No to well yeah lost I'm chily you cally  \n",
      "Soldow  \n",
      " year  \n",
      "Ustry hamon  \n",
      "  \n",
      "Another's neenight be notping on you thought  \n",
      "How bus  \n",
      "  \n",
      "Everything's not you you  \n",
      "I've callow on on yeah yed you are you been and out fuilly  \n",
      "Even lottis me  \n",
      "Here  \n",
      "Little my owfoer think oo \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 200000 iterations\n",
      "\n",
      " nd sing to through,  \n",
      "Lock you take me  \n",
      "Ffose in the wing baby, Down  \n",
      "  \n",
      "I king,  \n",
      "With the mami a litte that I say this  \n",
      "Of the withe me, byen there would be sometine  \n",
      "You  \n",
      "A newoe  \n",
      "Take to mak of the myghea row that's duston to ':5  \n",
      "Skee then so sweet a fripe on to seef this of  \n",
      "  \n",
      "Were yeah  \n",
      "I trie, that real  \n",
      "  \n",
      "Oh we time something a rememberes still make me  \n",
      "  \n",
      "To you back)  \n",
      "And wind  \n",
      "That she difioh, time this not list turduct end on right, I've go, I I never baby to flowin'  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 250000 iterations\n",
      "\n",
      " in the arm on my more to I amis onlearning and book of forgund a lone, don't you  \n",
      "  \n",
      "I wance to  \n",
      "So are in me you  \n",
      "If I wannind to feel at my mind  \n",
      "Whead see this something there,  \n",
      "Can't see  \n",
      "(Ind ly feod my prick and go somrow, enough me for me nothing lang  \n",
      "You mus hear, socrumss only brey from to mine.  \n",
      "Lay you  \n",
      "Could and to me  \n",
      "There's now  \n",
      "Now down...  \n",
      "Havurs  \n",
      "  \n",
      "And dom I'm can't her cry  \n",
      "We mick these you  \n",
      "Why wanna leaver  \n",
      "Than times is going to, think a many.., my slar   \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 300000 iterations\n",
      "\n",
      "  \n",
      "To shell, anyan of the beast where the can heapted there rusted  \n",
      "As to how must.  \n",
      "Sinver  \n",
      "To be alore, back  \n",
      "Feeld hisen hips wriffertay takes privel but the fadear dozy won't be a stry suok 't dots  \n",
      "Will alone again traps me thearce?  \n",
      "Take be dryather, them she? there it do your sound in the tears  \n",
      "Dunt romilarkon to mimas, to fhall that was loved  \n",
      "The long hand  \n",
      "It's dander your love tho dandorans high  \n",
      "Ever peadet get no to something that lemver to wascent?  \n",
      "And we tus what lorp  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 350000 iterations\n",
      "\n",
      " \n",
      "New  \n",
      "To be away, white everytolaned I beles wayhunitelsime  \n",
      "  \n",
      "Too morne  \n",
      "My fune  \n",
      "Done Mmg  \n",
      "In the for the fastlice  \n",
      "  \n",
      "Twow  \n",
      "Buson to the down my good Making  \n",
      "Dailf his fine  \n",
      "We're amands shonk I reals has for everybody burness  \n",
      "Tootortires of the feeling  \n",
      "It dies  \n",
      "Ona down I amens  \n",
      "Time on the faw  \n",
      "For life feet  \n",
      "And livetied not stwal Ies and jumpre astand one of hear the seope the more  \n",
      "  \n",
      "Everyway  \n",
      "  \n",
      "Not one fightarted on me emotied follidn't find home in away  \n",
      "  \n",
      "Done  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 400000 iterations\n",
      "\n",
      " ve is I worader  \n",
      "  \n",
      "That you go, I have a will are budngrs back hold? whan blaw tove a perserion  \n",
      "  \n",
      "[Chorus]  \n",
      "I was a mide  \n",
      "I have non-afny pall haicion-, I will can, (xo above shadory on ruin the scaty-now and but that's thing  \n",
      "Alle do  \n",
      "We rea-lazaM!  \n",
      "I wear  \n",
      "Do you want as I'm save god fline  \n",
      "A mo that with anow-ern't hears, sharhed to bus of my life yow ele,  \n",
      "Apparing like uh Man-no: and neak  \n",
      "  \n",
      "Non  \n",
      "So I am a stardied my man, lonely  \n",
      "Some of as you  \n",
      "All I'll bather tonce like \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    if pointer + seq_length+1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0  \n",
    "    \n",
    "    #select input sentence\n",
    "    input_sentence = data[pointer:pointer + seq_length]\n",
    "    \n",
    "    #select output sentence\n",
    "    output_sentence = data[pointer + 1:pointer + seq_length + 1]\n",
    "    \n",
    "    #get the indices of input and output sentence\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "\n",
    "    #convert the input and output sentence to a one-hot encoded vectors with the help of their indices\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "\n",
    "    \n",
    "    #train the network and get the final hidden state\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n",
    "                                      feed_dict={inputs: input_vector,targets: target_vector,init_state: hprev_val})\n",
    "   \n",
    "       \n",
    "    #make predictions on every 500th iteration \n",
    "    if iteration % 500 == 0:\n",
    "\n",
    "        #length of characters we want to predict\n",
    "        sample_length = 500\n",
    "        \n",
    "        #randomly select index\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "        \n",
    "        #sample the input sentence with the randomly selected index\n",
    "        sample_input_sent = data[random_index:random_index + seq_length]\n",
    "    \n",
    "        #get the indices of the sampled input sentence\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "        \n",
    "        #store the final hidden state in sample_prev_state_val\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "        \n",
    "        #for storing the indices of predicted characters\n",
    "        predicted_indices = []\n",
    "        \n",
    "        \n",
    "        for t in range(sample_length):\n",
    "            \n",
    "            #convert the sampled input sentence into one-hot encoded vector using their indices\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "            \n",
    "            #compute the probability of all the words in the vocabulary to be the next character\n",
    "            probs_dist, sample_prev_state_val = sess.run([output_softmax, hprev],\n",
    "                                                      feed_dict={inputs: sample_input_vector,init_state: sample_prev_state_val})\n",
    "\n",
    "            #we randomly select the index with the probabilty distribtuion generated by the model\n",
    "            ix = np.random.choice(range(vocab_size), p=probs_dist.ravel())\n",
    "            \n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "            \n",
    "            \n",
    "            #store the predicted index in predicted_indices list\n",
    "            predicted_indices.append(ix)\n",
    "            \n",
    "        #convert the predicted indices to their character\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "        \n",
    "        #combine the predcited characters\n",
    "        text = ''.join(predicted_chars)\n",
    "        \n",
    "        #predict the predict text on every 50000th iteration\n",
    "        if iteration %50000 == 0:           \n",
    "            print ('\\n')\n",
    "            print (' After %d iterations' %(iteration))\n",
    "            print('\\n %s \\n' % (text,))   \n",
    "            print('-'*115)\n",
    "\n",
    "            \n",
    "    #increment the pointer and iteration\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b61573-81b6-462b-9778-d9611a5d0c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf1_env)",
   "language": "python",
   "name": "tf1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
